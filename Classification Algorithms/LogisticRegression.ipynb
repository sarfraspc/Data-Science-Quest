{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression\n",
    "\n",
    "## `LogisticRegression` Parameters\n",
    "\n",
    "| **Param**           | **Default** | **What it does**                                                                               |\n",
    "|------------------------|------------------------|------------------------|\n",
    "| `penalty`           | `'l2'`      | Type of regularization: `'l1'`, `'l2'`, `'elasticnet'`, or `'none'`. Helps avoid overfitting.  |\n",
    "| `dual`              | `False`     | Use the **dual formulation** (only for `liblinear` + `l2`). Mostly leave it `False`.           |\n",
    "| `tol`               | `1e-4`      | Tolerance for stopping criteria. Smaller = more precise, but slower.                           |\n",
    "| `C`                 | `1.0`       | Inverse of regularization strength. Smaller = more regularization (like alpha in Ridge/Lasso). |\n",
    "| `fit_intercept`     | `True`      | Whether to fit the bias term (intercept). Almost always `True`.                                |\n",
    "| `intercept_scaling` | `1`         | Only matters when `solver='liblinear'` and `fit_intercept=True`.                               |\n",
    "| `class_weight`      | `None`      | Handle imbalanced classes: `'balanced'` or dict.                                               |\n",
    "| `random_state`      | `None`      | Seed for reproducibility. Only relevant for `saga`, `liblinear`, etc.                          |\n",
    "| `solver`            | `'lbfgs'`   | Optimizer used to fit the model: `'liblinear'`, `'lbfgs'`, `'newton-cg'`, `'saga'`, `'sag'`.   |\n",
    "| `max_iter`          | `100`       | Max iterations before giving up on convergence. Increase if model is stubborn.                 |\n",
    "| `multi_class`       | `'auto'`    | `'ovr'` (One-vs-Rest) or `'multinomial'`. `'auto'` picks best based on solver.                 |\n",
    "| `verbose`           | `0`         | Prints optimization info. Good for debugging.                                                  |\n",
    "| `warm_start`        | `False`     | Reuse previous solution to speed up training (useful in cross-validation or loops).            |\n",
    "| `n_jobs`            | `None`      | Parallel computation (only for `liblinear`). Set to `-1` to use all cores.                     |\n",
    "| `l1_ratio`          | `None`      | Only for `penalty='elasticnet'` + `solver='saga'`. Controls balance between L1 and L2.         |\n",
    "\n",
    "Example on 3 different datasets\n",
    "\n",
    "-   Titanic\n",
    "\n",
    "    ``` python\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Load Titanic dataset (Seaborn provides it)\n",
    "    titanic = sns.load_dataset('titanic')\n",
    "\n",
    "    # Data Preprocessing\n",
    "    titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
    "    titanic.dropna(subset=['embarked'], inplace=True)\n",
    "\n",
    "    # Encoding 'sex' using LabelEncoder (male = 0, female = 1)\n",
    "    titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "    # One-Hot Encoding 'embarked' and 'pclass'\n",
    "    titanic = pd.get_dummies(titanic, columns=['embarked', 'pclass'], drop_first=True)\n",
    "\n",
    "    # Features (X) and Target (y)\n",
    "    X = titanic[['age', 'sibsp', 'parch', 'fare', 'sex', 'embarked_Q', 'embarked_S']]\n",
    "    y = titanic['survived']\n",
    "\n",
    "    # Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Split Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train Logistic Regression Model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions and Evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Classification Report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    ```\n",
    "\n",
    "-   Iris\n",
    "\n",
    "    ``` python\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Load dataset\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['target'] = iris.target\n",
    "    df['species'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "    # Features and target\n",
    "    X = df[iris.feature_names]\n",
    "    y = df['target']\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(conf_matrix, annot=True, cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    # Classification report\n",
    "    print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "    ```\n",
    "\n",
    "-   Breast Cancer Wisconsin\n",
    "\n",
    "    ``` python\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv('breast_cancer.csv')\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed|id', case=False)]\n",
    "\n",
    "    # Encode target\n",
    "    df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "    # Features and target\n",
    "    X = df.drop('diagnosis', axis=1)\n",
    "    y = df['diagnosis']\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict & evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(conf_matrix, annot=True, cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_test, y_pred, target_names=['Benign', 'Malignant']))\n",
    "    ```\n",
    "\n",
    "| Use Case              | How Logistic Regression Handles It                                 |\n",
    "|------------------------------------|------------------------------------|\n",
    "| Binary classification | ✅ Out-of-the-box                                                  |\n",
    "| Multiclass (basic)    | ✅ With One-vs-Rest (OvR)                                          |\n",
    "| Multiclass (better)   | ✅ With `multi_class='multinomial'` and `solver='lbfgs'` or `saga` |"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
