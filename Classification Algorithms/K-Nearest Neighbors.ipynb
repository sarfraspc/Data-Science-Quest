{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "### **K-Nearest Neighbors (KNN): Hyperparameter Tuning**\n",
    "\n",
    "**Hyperparameters to tune**:\n",
    "\n",
    "-   **`n_neighbors`**: The number of neighbors to consider when making a\n",
    "    prediction.\n",
    "-   **`weights`**: How the neighbors’ votes are weighted (e.g.,\n",
    "    `'uniform'`, `'distance'`).\n",
    "-   **`metric`**: The distance metric (e.g., `'euclidean'`,\n",
    "    `'manhattan'`, `'minkowski'`).\n",
    "-   **`algorithm`**: The algorithm used to compute the nearest neighbors\n",
    "    (`'auto'`, `'ball_tree'`, `'kd_tree'`, `'brute'`).\n",
    "-   **`p`**: Power parameter for the Minkowski distance metric\n",
    "    (typically 2 for Euclidean distance).\n",
    "-   **`leaf_size`**: Affects the speed of tree-based algorithms (only\n",
    "    for `ball_tree` or `kd_tree`).\n",
    "\n",
    "**Tuning Method**:\n",
    "\n",
    "-   **Grid Search**: Test different values for `n_neighbors`, `weights`,\n",
    "    `metric`, etc.\n",
    "-   **Randomized Search**: Explore a wide range of hyperparameters\n",
    "    quickly.\n",
    "\n",
    "**Technique**:\n",
    "\n",
    "-   **GridSearchCV** or **RandomizedSearchCV** from\n",
    "    `sklearn.model_selection`.\n",
    "-   **Cross-validation**: Use K-fold cross-validation to find the\n",
    "    optimal values that generalize well.\n",
    "\n",
    "## Advantages\n",
    "\n",
    "| Good Stuff                                 | Why It Rocks                              |\n",
    "|------------------------------------|------------------------------------|\n",
    "| **No training time**                       | Stores data, done!                        |\n",
    "| **Simple & intuitive**                     | Neighbors vote — that’s it                |\n",
    "| **Works with any number of classes**       | Binary, multiclass — no problem           |\n",
    "| **Adapts to data**                         | Complex boundaries? It just works         |\n",
    "| **No assumptions about data distribution** | Unlike Naive Bayes or Logistic Regression |\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "| Bad Stuff                            | Why It Hurts                                     |\n",
    "|------------------------------------|------------------------------------|\n",
    "| **Slow at prediction time**          | Has to search the whole training set every time! |\n",
    "| **Memory hog**                       | Stores entire dataset in RAM                     |\n",
    "| **Sensitive to irrelevant features** | You better scale or select features well         |\n",
    "| **Needs scaling (standardization)**  | Because distances are affected by scale          |\n",
    "| **K selection is tricky**            | Too low? Noisy. Too high? Biased.                |\n",
    "\n",
    "-   Iris\n",
    "\n",
    "    ``` python\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Load data\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "\n",
    "    # Scale\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Model\n",
    "    model = KNeighborsClassifier(n_neighbors=5)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict & Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Iris Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "    plt.title(\"Confusion Matrix - Iris\")\n",
    "    plt.show()\n",
    "    print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "    ```\n",
    "\n",
    "-   Breast Cancer Wisconsin\n",
    "\n",
    "    ``` python\n",
    "    df = pd.read_csv('breast_cancer.csv')\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed|id', case=False)]\n",
    "    df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "    X = df.drop('diagnosis', axis=1)\n",
    "    y = df['diagnosis']\n",
    "\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=5)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Breast Cancer Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
    "    plt.title(\"Confusion Matrix - Breast Cancer\")\n",
    "    plt.show()\n",
    "    print(classification_report(y_test, y_pred, target_names=['Benign', 'Malignant']))\n",
    "    ```\n",
    "\n",
    "-   Titanic\n",
    "\n",
    "    ``` python\n",
    "    df = pd.read_csv('titanic.csv')\n",
    "\n",
    "    # Sample preprocessing (tweak as needed)\n",
    "    df = df[['Pclass', 'Sex', 'Age', 'Fare', 'Survived']]\n",
    "    df.dropna(inplace=True)\n",
    "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "    X = df.drop('Survived', axis=1)\n",
    "    y = df['Survived']\n",
    "\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=5)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Titanic Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', xticklabels=['Died', 'Survived'], yticklabels=['Died', 'Survived'])\n",
    "    plt.title(\"Confusion Matrix - Titanic\")\n",
    "    plt.show()\n",
    "    print(classification_report(y_test, y_pred, target_names=['Died', 'Survived']))\n",
    "    ```"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
