{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier\n",
    "\n",
    "### Load & Explore IRIS Dataset\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load iris\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Train-Test Split\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Train the Decision Tree\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Evaluate It\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Visualize the Tree\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(dt, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Save Model\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "import joblib\n",
    "joblib.dump(dt, \"dt_iris.pkl\")\n",
    "```\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "### üîπ What is a Decision Tree?\n",
    "\n",
    "A **Decision Tree** is a supervised ML algorithm used for\n",
    "**classification** and **regression**.\n",
    "\n",
    "It mimics human decision-making using a tree-like model of decisions.\n",
    "\n",
    "üëâ Each internal node: a feature (condition)\n",
    "\n",
    "üëâ Each branch: outcome of the condition\n",
    "\n",
    "üëâ Each leaf: final decision/class\n",
    "\n",
    "> Think of it like playing 20 Questions with your data.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### üîπ Real-World Analogy:\n",
    "\n",
    "Imagine you‚Äôre picking a fruit:\n",
    "\n",
    "-   Is it red?\n",
    "\n",
    "    ‚Üí Yes ‚Üí Is it round?\n",
    "\n",
    "    ‚Üí Yes ‚Üí Apple\n",
    "\n",
    "    ‚Üí No ‚Üí Strawberry\n",
    "\n",
    "-   Is it yellow?\n",
    "\n",
    "    ‚Üí Banana!\n",
    "\n",
    "Simple if-else checks! That‚Äôs a decision tree.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### üîπ Types of Splitting Criteria (How it Decides the Best Question):\n",
    "\n",
    "When building a tree, it chooses questions that give the **purest\n",
    "split** using:\n",
    "\n",
    "### 1. **Gini Impurity**\n",
    "\n",
    "-   detailed\n",
    "\n",
    "    \\### What it means:\n",
    "\n",
    "    Measures how **impure** a node is. Lower Gini = purer node = better\n",
    "    split!\n",
    "\n",
    "    > Formula:\n",
    "\n",
    "    Gini = 1 - p_i^2\n",
    "\n",
    "    \\]\n",
    "\n",
    "    Where pip_ipi is the probability of class **i** at the node.\n",
    "\n",
    "    \\### Example:\n",
    "\n",
    "    If a node has 50% Class A and 50% Class B:\n",
    "\n",
    "    Gini=1‚àí(0.52+0.52)=0.5Gini = 1 - (0.5^2 + 0.5^2) = 0.5\n",
    "\n",
    "    Gini=1‚àí(0.52+0.52)=0.5\n",
    "\n",
    "    If a node has 100% Class A:\n",
    "\n",
    "    Gini=1‚àí12=0(pure!)Gini = 1 - 1^2 = 0 (pure!)\n",
    "\n",
    "    Gini=1‚àí12=0(pure!)\n",
    "\n",
    "    \\### Use:\n",
    "\n",
    "    -   Fast to compute\n",
    "    -   Default in **sklearn**\n",
    "\n",
    "-   Measures impurity of a node.\n",
    "\n",
    "-   Range: 0 (pure) to max of \\~0.5 (for binary).\n",
    "\n",
    "### 2. **Entropy (Information Gain)**\n",
    "\n",
    "-   detailed\n",
    "\n",
    "    \\### Entropy:\n",
    "\n",
    "    Measures **uncertainty** or **disorder**.\n",
    "\n",
    "    \\### Information Gain:\n",
    "\n",
    "    Difference in entropy **before** and **after** the split.\n",
    "\n",
    "    IG=Entropy(parent)‚àíWeighted¬†Avg¬†Entropy¬†of¬†children\n",
    "\n",
    "    \\### Example:\n",
    "\n",
    "    If a node is mixed (like 60% yes, 40% no), entropy is high.\n",
    "\n",
    "    If it‚Äôs all yes or all no ‚Üí entropy is zero.\n",
    "\n",
    "    \\### Use:\n",
    "\n",
    "    -   More **theoretically grounded** (from Information Theory)\n",
    "    -   Slightly slower than Gini\n",
    "    -   Good for **feature selection**\n",
    "\n",
    "-   Think of it as disorder\n",
    "\n",
    "-   Info Gain = Entropy(Parent) - Weighted Entropy(Children)\n",
    "\n",
    "-   More computationally intense than Gini.\n",
    "\n",
    "**so inoformation gain other name is entropy**\n",
    "\n",
    "No, Information Gain and Entropy are not the same thing, but they are\n",
    "very closely related concepts in the context of Decision Trees.\n",
    "\n",
    "Here‚Äôs the distinction:\n",
    "\n",
    "-   **Entropy (H):**\n",
    "    -   **What it is:** Entropy is a **measure of impurity, disorder, or\n",
    "        uncertainty** within a set of data.\n",
    "    -   **How it works:** In a decision tree, for a given node, entropy\n",
    "        quantifies how mixed the class labels are.\n",
    "        -   If all data points in a node belong to the same class\n",
    "            (perfectly pure), the entropy is 0.\n",
    "        -   If the data points are evenly distributed across multiple\n",
    "            classes (maximum impurity/uncertainty), the entropy is high\n",
    "            (e.g., 1 for a binary classification problem with 50/50\n",
    "            split).\n",
    "    -   **Purpose:** It tells you how ‚Äúmessy‚Äù a node is before and after\n",
    "        a potential split.\n",
    "-   **Information Gain (IG):**\n",
    "    -   **What it is:** Information Gain is the **reduction in entropy**\n",
    "        achieved by splitting the data on a particular attribute\n",
    "        (feature). It measures how much ‚Äúinformation‚Äù a feature provides\n",
    "        about the class labels.\n",
    "    -   **How it works:** You calculate the entropy of the parent node\n",
    "        (Hparent) and then the weighted average entropy of the child\n",
    "        nodes after the split (Hchildren). The Information Gain is the\n",
    "        difference: Where:\n",
    "        -   S is the parent set of examples.\n",
    "        -   A is the attribute being split on.\n",
    "        -   Values(A) are the possible values of attribute A.\n",
    "        -   Sv is the subset of S for which attribute A has value v.\n",
    "        -   ‚à£S‚à£‚à£Sv‚à£ is the proportion of examples in S that have value v\n",
    "            for attribute A.\n",
    "    -   **Purpose:** The decision tree algorithm uses Information Gain\n",
    "        to select the best feature to split on at each step. It chooses\n",
    "        the feature that provides the **highest Information Gain**,\n",
    "        meaning it most effectively reduces the overall impurity of the\n",
    "        dataset.\n",
    "\n",
    "**Analogy:**\n",
    "\n",
    "Imagine you have a bag of marbles, some red and some blue.\n",
    "\n",
    "-   **Entropy** is how mixed up the colors are in the bag. If you have\n",
    "    equal numbers of red and blue, the entropy is high (very mixed). If\n",
    "    you only have red marbles, the entropy is low (not mixed at all).\n",
    "-   **Information Gain** is the benefit you get from sorting the\n",
    "    marbles. If you have a way to pick up a handful of marbles and\n",
    "    suddenly they are all red, you‚Äôve gained a lot of ‚Äúinformation‚Äù\n",
    "    about their color, and the ‚Äúimpurity‚Äù of that handful has\n",
    "    drastically reduced. The process of splitting the data by a feature\n",
    "    is like finding a way to sort the marbles and reduce their\n",
    "    mixed-up-ness.\n",
    "\n",
    "üß† **TL;DR**: Both measure ‚Äúpurity‚Äù. Gini is faster; Entropy is more\n",
    "precise.\n",
    "\n",
    "**which to use which**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ü•á **Gini Impurity**\n",
    "\n",
    "‚úÖ **Faster to compute** (no logarithms!)\n",
    "\n",
    "‚úÖ Works great in practice\n",
    "\n",
    "‚úÖ Default in `sklearn`‚Äôs `DecisionTreeClassifier`\n",
    "\n",
    "‚ùå Doesn‚Äôt have the theoretical ‚Äúpurity‚Äù of entropy\n",
    "\n",
    "‚ùå Slightly less sensitive to class imbalance\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ü•à **Information Gain (Entropy)**\n",
    "\n",
    "‚úÖ Based on **Information Theory**\n",
    "\n",
    "‚úÖ More **interpretable** for humans\n",
    "\n",
    "‚úÖ Slightly better for **imbalanced datasets** or when split quality\n",
    "needs precision\n",
    "\n",
    "‚ùå Slower (due to logs)\n",
    "\n",
    "‚ùå Can favor attributes with **many categories** (unless you use Gain\n",
    "Ratio)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ‚öîÔ∏è So‚Ä¶ **Which to use?**\n",
    "\n",
    "| Situation                         | Recommendation                   |\n",
    "|-----------------------------------|----------------------------------|\n",
    "| You‚Äôre using `sklearn`            | Stick with **Gini** (default) ‚úÖ |\n",
    "| You want theory-based purity      | Try **Information Gain** üß†      |\n",
    "| Data is **highly imbalanced**     | IG might edge out Gini üí•        |\n",
    "| You‚Äôre aiming for **speed**       | Gini is faster üèÉ‚Äç‚ôÇÔ∏è                |\n",
    "| You want to impress in interviews | Know **both**! üòéüéì              |\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### üéØ Final Verdict (Practical Advice):\n",
    "\n",
    "> Use Gini by default (fast, solid, works well).\n",
    ">\n",
    "> Try **Information Gain** if your tree seems weird, you‚Äôre dealing with\n",
    "> **imbalanced data**, or you‚Äôre just curious to compare!\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### üîπ DecisionTreeClassifier in `sklearn`\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### üîπ Visualizing the Tree\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plot_tree(model, feature_names=X.columns, class_names=['No','Yes'], filled=True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Or export it nicely:\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.tree import export_text\n",
    "print(export_text(model, feature_names=list(X.columns)))\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### üîπ Important Parameters\n",
    "\n",
    "| Parameter           | Description                                               |\n",
    "|------------------------------------|------------------------------------|\n",
    "| `criterion`         | ‚Äògini‚Äô (default) or ‚Äòentropy‚Äô                             |\n",
    "| `max_depth`         | Max levels in the tree                                    |\n",
    "| `min_samples_split` | Min samples needed to split                               |\n",
    "| `min_samples_leaf`  | Min samples in a leaf                                     |\n",
    "| `max_features`      | How many features to consider when looking for best split |\n",
    "| `random_state`      | Reproducibility                                           |\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### üîπ Pros vs Cons\n",
    "\n",
    "‚úÖ **Pros**:\n",
    "\n",
    "-   Easy to understand/visualize\n",
    "-   No need for feature scaling\n",
    "-   Handles both numeric and categorical data\n",
    "\n",
    "‚ùå **Cons**:\n",
    "\n",
    "-   Prone to overfitting (deep trees)\n",
    "-   Sensitive to data noise\n",
    "-   Less accurate than ensemble methods (like Random Forest)\n",
    "\n",
    "# Applications of Decision Trees\n",
    "\n",
    "-   **Loan Approval in Banking**: A bank needs to decide whether to\n",
    "    approve a loan application based on customer profiles.\n",
    "    -   Input features include income, credit score, employment status,\n",
    "        and loan history.\n",
    "    -   The decision tree predicts loan approval or rejection, helping\n",
    "        the bank make quick and reliable decisions.\n",
    "-   **Medical Diagnosis:**¬†A healthcare provider wants to predict\n",
    "    whether a patient has diabetes based on clinical test results.\n",
    "    -   Features like glucose levels, BMI, and blood pressure are used\n",
    "        to make a decision tree.\n",
    "    -   Tree classifies patients into diabetic or non-diabetic,\n",
    "        assisting doctors in diagnosis.\n",
    "-   **Predicting Exam Results in Education : S**chool wants to predict\n",
    "    whether a student will pass or fail based on study habits.\n",
    "    -   Data includes attendance, time spent studying, and previous\n",
    "        grades.\n",
    "    -   The decision tree identifies at-risk students, allowing teachers\n",
    "        to provide additional support.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### üîπ Mini Project Ideas\n",
    "\n",
    "-   üç∑ Classify wine quality using `sklearn.datasets.load_wine()`\n",
    "-   üß¨ Predict cancer from `load_breast_cancer()`\n",
    "-   üö¢ Survival prediction with Titanic dataset\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### üîπ Common Mistakes to Avoid\n",
    "\n",
    "-   Not setting `max_depth` ‚Üí overfitting alert üö®\n",
    "-   Using unscaled data? No worries, DT doesn‚Äôt care üòé\n",
    "-   Confusing **Gini** and **Entropy**‚Äîthey‚Äôre *friends*, not foes.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## üõ°Ô∏è Techniques to Prevent Overfitting in Decision Trees\n",
    "\n",
    "Let‚Äôs **chop the tree wisely** üå≥‚úÇÔ∏è\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 1. **`max_depth`**\n",
    "\n",
    "> Limit how deep the tree can grow.\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=4)\n",
    "```\n",
    "\n",
    "ü™ö Stops tree from splitting forever.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 2. **`min_samples_split`**\n",
    "\n",
    "> Minimum samples needed to split a node.\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "DecisionTreeClassifier(min_samples_split=10)\n",
    "```\n",
    "\n",
    "üìâ Bigger number = fewer splits = simpler tree.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 3. **`min_samples_leaf`**\n",
    "\n",
    "> Minimum samples required in a leaf node.\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "DecisionTreeClassifier(min_samples_leaf=5)\n",
    "```\n",
    "\n",
    "üåø Prevents tiny, overfitted leaves with 1 or 2 rows.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 4. **`max_leaf_nodes`**\n",
    "\n",
    "> Restrict number of leaf nodes (final decisions)\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "DecisionTreeClassifier(max_leaf_nodes=15)\n",
    "```\n",
    "\n",
    "üå≤ Keeps it compact.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 5. **`max_features`**\n",
    "\n",
    "> Randomly select only some features per split.\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "DecisionTreeClassifier(max_features=\"sqrt\")\n",
    "```\n",
    "\n",
    "üåà Adds randomness ‚Üí good for ensembles too.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 6. **Pruning** (Post-training trimming)\n",
    "\n",
    "In `sklearn`, use **Cost Complexity Pruning**:\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "DecisionTreeClassifier(ccp_alpha=0.01)\n",
    "```\n",
    "\n",
    "‚úÇÔ∏è Removes branches that don‚Äôt improve performance.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 7. **Cross-validation**\n",
    "\n",
    "> Tune hyperparameters using GridSearchCV or RandomizedSearchCV to find\n",
    "> sweet spot üéØ\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### üîÆ BONUS: Use Random Forests üå≥üå≥üå≥\n",
    "\n",
    "Random Forest = **lots of shallow, random trees**\n",
    "\n",
    "‚úÖ Way more resistant to overfitting than a single deep tree.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## üß† TL;DR Cheat Sheet:\n",
    "\n",
    "| Parameter           | What it controls | Helps with     |\n",
    "|---------------------|------------------|----------------|\n",
    "| `max_depth`         | Tree depth       | Complexity     |\n",
    "| `min_samples_split` | When to split    | Over-splitting |\n",
    "| `min_samples_leaf`  | Small leaves     | Noise          |\n",
    "| `ccp_alpha`         | Post-pruning     | Tiny tweaks    |\n",
    "| `max_leaf_nodes`    | Limits decisions | Simplicity     |"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
