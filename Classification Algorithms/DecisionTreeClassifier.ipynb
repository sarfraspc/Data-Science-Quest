{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier\n",
    "\n",
    "### Load & Explore IRIS Dataset\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load iris\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Train-Test Split\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Train the Decision Tree\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Evaluate It\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Visualize the Tree\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(dt, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Save Model\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "import joblib\n",
    "joblib.dump(dt, \"dt_iris.pkl\")\n",
    "```\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "### ðŸ”¹ What is a Decision Tree?\n",
    "\n",
    "A **Decision Tree** is a supervised ML algorithm used for\n",
    "**classification** and **regression**.\n",
    "\n",
    "It mimics human decision-making using a tree-like model of decisions.\n",
    "\n",
    "ðŸ‘‰ Each internal node: a feature (condition)\n",
    "\n",
    "ðŸ‘‰ Each branch: outcome of the condition\n",
    "\n",
    "ðŸ‘‰ Each leaf: final decision/class\n",
    "\n",
    "> Think of it like playing 20 Questions with your data.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ðŸ”¹ Real-World Analogy:\n",
    "\n",
    "Imagine youâ€™re picking a fruit:\n",
    "\n",
    "-   Is it red?\n",
    "\n",
    "    â†’ Yes â†’ Is it round?\n",
    "\n",
    "    â†’ Yes â†’ Apple\n",
    "\n",
    "    â†’ No â†’ Strawberry\n",
    "\n",
    "-   Is it yellow?\n",
    "\n",
    "    â†’ Banana!\n",
    "\n",
    "Simple if-else checks! Thatâ€™s a decision tree.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ðŸ”¹ Types of Splitting Criteria (How it Decides the Best Question):\n",
    "\n",
    "When building a tree, it chooses questions that give the **purest\n",
    "split** using:\n",
    "\n",
    "### 1. **Gini Impurity**\n",
    "\n",
    "-   detailed\n",
    "\n",
    "    \\### What it means:\n",
    "\n",
    "    Measures how **impure** a node is. Lower Gini = purer node = better\n",
    "    split!\n",
    "\n",
    "    > Formula:\n",
    "\n",
    "    Gini = 1 - p_i^2\n",
    "\n",
    "    \\]\n",
    "\n",
    "    Where pip_ipi is the probability of class **i** at the node.\n",
    "\n",
    "    \\### Example:\n",
    "\n",
    "    If a node has 50% Class A and 50% Class B:\n",
    "\n",
    "    Gini=1âˆ’(0.52+0.52)=0.5Gini = 1 - (0.5^2 + 0.5^2) = 0.5\n",
    "\n",
    "    Gini=1âˆ’(0.52+0.52)=0.5\n",
    "\n",
    "    If a node has 100% Class A:\n",
    "\n",
    "    Gini=1âˆ’12=0(pure!)Gini = 1 - 1^2 = 0 (pure!)\n",
    "\n",
    "    Gini=1âˆ’12=0(pure!)\n",
    "\n",
    "    \\### Use:\n",
    "\n",
    "    -   Fast to compute\n",
    "    -   Default in **sklearn**\n",
    "\n",
    "-   Measures impurity of a node.\n",
    "\n",
    "-   Range: 0 (pure) to max of \\~0.5 (for binary).\n",
    "\n",
    "### 2. **Entropy (Information Gain)**\n",
    "\n",
    "-   detailed\n",
    "\n",
    "    \\### Entropy:\n",
    "\n",
    "    Measures **uncertainty** or **disorder**.\n",
    "\n",
    "    \\### Information Gain:\n",
    "\n",
    "    Difference in entropy **before** and **after** the split.\n",
    "\n",
    "    IG=Entropy(parent)âˆ’WeightedÂ AvgÂ EntropyÂ ofÂ children\n",
    "\n",
    "    \\### Example:\n",
    "\n",
    "    If a node is mixed (like 60% yes, 40% no), entropy is high.\n",
    "\n",
    "    If itâ€™s all yes or all no â†’ entropy is zero.\n",
    "\n",
    "    \\### Use:\n",
    "\n",
    "    -   More **theoretically grounded** (from Information Theory)\n",
    "    -   Slightly slower than Gini\n",
    "    -   Good for **feature selection**\n",
    "\n",
    "-   Think of it as disorder\n",
    "\n",
    "-   Info Gain = Entropy(Parent) - Weighted Entropy(Children)\n",
    "\n",
    "-   More computationally intense than Gini.\n",
    "\n",
    "**so inoformation gain other name is entropy**\n",
    "\n",
    "No, Information Gain and Entropy are not the same thing, but they are\n",
    "very closely related concepts in the context of Decision Trees.\n",
    "\n",
    "Hereâ€™s the distinction:\n",
    "\n",
    "-   **Entropy (H):**\n",
    "    -   **What it is:** Entropy is a **measure of impurity, disorder, or\n",
    "        uncertainty** within a set of data.\n",
    "    -   **How it works:** In a decision tree, for a given node, entropy\n",
    "        quantifies how mixed the class labels are.\n",
    "        -   If all data points in a node belong to the same class\n",
    "            (perfectly pure), the entropy is 0.\n",
    "        -   If the data points are evenly distributed across multiple\n",
    "            classes (maximum impurity/uncertainty), the entropy is high\n",
    "            (e.g., 1 for a binary classification problem with 50/50\n",
    "            split).\n",
    "    -   **Purpose:** It tells you how â€œmessyâ€ a node is before and after\n",
    "        a potential split.\n",
    "-   **Information Gain (IG):**\n",
    "    -   **What it is:** Information Gain is the **reduction in entropy**\n",
    "        achieved by splitting the data on a particular attribute\n",
    "        (feature). It measures how much â€œinformationâ€ a feature provides\n",
    "        about the class labels.\n",
    "    -   **How it works:** You calculate the entropy of the parent node\n",
    "        (Hparent) and then the weighted average entropy of the child\n",
    "        nodes after the split (Hchildren). The Information Gain is the\n",
    "        difference: Where:\n",
    "        -   S is the parent set of examples.\n",
    "        -   A is the attribute being split on.\n",
    "        -   Values(A) are the possible values of attribute A.\n",
    "        -   Sv is the subset of S for which attribute A has value v.\n",
    "        -   âˆ£Sâˆ£âˆ£Svâˆ£ is the proportion of examples in S that have value v\n",
    "            for attribute A.\n",
    "    -   **Purpose:** The decision tree algorithm uses Information Gain\n",
    "        to select the best feature to split on at each step. It chooses\n",
    "        the feature that provides the **highest Information Gain**,\n",
    "        meaning it most effectively reduces the overall impurity of the\n",
    "        dataset.\n",
    "\n",
    "**Analogy:**\n",
    "\n",
    "Imagine you have a bag of marbles, some red and some blue.\n",
    "\n",
    "-   **Entropy** is how mixed up the colors are in the bag. If you have\n",
    "    equal numbers of red and blue, the entropy is high (very mixed). If\n",
    "    you only have red marbles, the entropy is low (not mixed at all).\n",
    "-   **Information Gain** is the benefit you get from sorting the\n",
    "    marbles. If you have a way to pick up a handful of marbles and\n",
    "    suddenly they are all red, youâ€™ve gained a lot of â€œinformationâ€\n",
    "    about their color, and the â€œimpurityâ€ of that handful has\n",
    "    drastically reduced. The process of splitting the data by a feature\n",
    "    is like finding a way to sort the marbles and reduce their\n",
    "    mixed-up-ness.\n",
    "\n",
    "ðŸ§  **TL;DR**: Both measure â€œpurityâ€. Gini is faster; Entropy is more\n",
    "precise.\n",
    "\n",
    "**which to use which**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ðŸ¥‡ **Gini Impurity**\n",
    "\n",
    "âœ… **Faster to compute** (no logarithms!)\n",
    "\n",
    "âœ… Works great in practice\n",
    "\n",
    "âœ… Default in `sklearn`â€™s `DecisionTreeClassifier`\n",
    "\n",
    "âŒ Doesnâ€™t have the theoretical â€œpurityâ€ of entropy\n",
    "\n",
    "âŒ Slightly less sensitive to class imbalance\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ðŸ¥ˆ **Information Gain (Entropy)**\n",
    "\n",
    "âœ… Based on **Information Theory**\n",
    "\n",
    "âœ… More **interpretable** for humans\n",
    "\n",
    "âœ… Slightly better for **imbalanced datasets** or when split quality\n",
    "needs precision\n",
    "\n",
    "âŒ Slower (due to logs)\n",
    "\n",
    "âŒ Can favor attributes with **many categories** (unless you use Gain\n",
    "Ratio)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### âš”ï¸ Soâ€¦ **Which to use?**\n",
    "\n",
    "| Situation                         | Recommendation                   |\n",
    "|-----------------------------------|----------------------------------|\n",
    "| Youâ€™re using `sklearn`            | Stick with **Gini** (default) âœ… |\n",
    "| You want theory-based purity      | Try **Information Gain** ðŸ§       |\n",
    "| Data is **highly imbalanced**     | IG might edge out Gini ðŸ’¥        |\n",
    "| Youâ€™re aiming for **speed**       | Gini is faster ðŸƒâ€â™‚ï¸                |\n",
    "| You want to impress in interviews | Know **both**! ðŸ˜ŽðŸŽ“              |\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ðŸŽ¯ Final Verdict (Practical Advice):\n",
    "\n",
    "> Use Gini by default (fast, solid, works well).\n",
    ">\n",
    "> Try **Information Gain** if your tree seems weird, youâ€™re dealing with\n",
    "> **imbalanced data**, or youâ€™re just curious to compare!\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ðŸ”¹ DecisionTreeClassifier in `sklearn`\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ðŸ”¹ Visualizing the Tree\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plot_tree(model, feature_names=X.columns, class_names=['No','Yes'], filled=True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Or export it nicely:\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.tree import export_text\n",
    "print(export_text(model, feature_names=list(X.columns)))\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ðŸ”¹ Important Parameters\n",
    "\n",
    "| Parameter           | Description                                               |\n",
    "|------------------------------------|------------------------------------|\n",
    "| `criterion`         | â€˜giniâ€™ (default) or â€˜entropyâ€™                             |\n",
    "| `max_depth`         | Max levels in the tree                                    |\n",
    "| `min_samples_split` | Min samples needed to split                               |\n",
    "| `min_samples_leaf`  | Min samples in a leaf                                     |\n",
    "| `max_features`      | How many features to consider when looking for best split |\n",
    "| `random_state`      | Reproducibility                                           |\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ðŸ”¹ Pros vs Cons\n",
    "\n",
    "âœ… **Pros**:\n",
    "\n",
    "-   Easy to understand/visualize\n",
    "-   No need for feature scaling\n",
    "-   Handles both numeric and categorical data\n",
    "\n",
    "âŒ **Cons**:\n",
    "\n",
    "-   Prone to overfitting (deep trees)\n",
    "-   Sensitive to data noise\n",
    "-   Less accurate than ensemble methods (like Random Forest)\n",
    "\n",
    "# Applications of Decision Trees\n",
    "\n",
    "-   **Loan Approval in Banking**: A bank needs to decide whether to\n",
    "    approve a loan application based on customer profiles.\n",
    "    -   Input features include income, credit score, employment status,\n",
    "        and loan history.\n",
    "    -   The decision tree predicts loan approval or rejection, helping\n",
    "        the bank make quick and reliable decisions.\n",
    "-   **Medical Diagnosis:**Â A healthcare provider wants to predict\n",
    "    whether a patient has diabetes based on clinical test results.\n",
    "    -   Features like glucose levels, BMI, and blood pressure are used\n",
    "        to make a decision tree.\n",
    "    -   Tree classifies patients into diabetic or non-diabetic,\n",
    "        assisting doctors in diagnosis.\n",
    "-   **Predicting Exam Results in Education : S**chool wants to predict\n",
    "    whether a student will pass or fail based on study habits.\n",
    "    -   Data includes attendance, time spent studying, and previous\n",
    "        grades.\n",
    "    -   The decision tree identifies at-risk students, allowing teachers\n",
    "        to provide additional support.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ðŸ”¹ Mini Project Ideas\n",
    "\n",
    "-   ðŸ· Classify wine quality using `sklearn.datasets.load_wine()`\n",
    "-   ðŸ§¬ Predict cancer from `load_breast_cancer()`\n",
    "-   ðŸš¢ Survival prediction with Titanic dataset\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ðŸ”¹ Common Mistakes to Avoid\n",
    "\n",
    "-   Not setting `max_depth` â†’ overfitting alert ðŸš¨\n",
    "-   Using unscaled data? No worries, DT doesnâ€™t care ðŸ˜Ž\n",
    "-   Confusing **Gini** and **Entropy**â€”theyâ€™re *friends*, not foes.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## ðŸ›¡ï¸ Techniques to Prevent Overfitting in Decision Trees\n",
    "\n",
    "Letâ€™s **chop the tree wisely** ðŸŒ³âœ‚ï¸\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 1. **`max_depth`**\n",
    "\n",
    "> Limit how deep the tree can grow.\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=4)\n",
    "```\n",
    "\n",
    "ðŸªš Stops tree from splitting forever.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 2. **`min_samples_split`**\n",
    "\n",
    "> Minimum samples needed to split a node.\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "DecisionTreeClassifier(min_samples_split=10)\n",
    "```\n",
    "\n",
    "ðŸ“‰ Bigger number = fewer splits = simpler tree.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 3. **`min_samples_leaf`**\n",
    "\n",
    "> Minimum samples required in a leaf node.\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "DecisionTreeClassifier(min_samples_leaf=5)\n",
    "```\n",
    "\n",
    "ðŸŒ¿ Prevents tiny, overfitted leaves with 1 or 2 rows.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 4. **`max_leaf_nodes`**\n",
    "\n",
    "> Restrict number of leaf nodes (final decisions)\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "DecisionTreeClassifier(max_leaf_nodes=15)\n",
    "```\n",
    "\n",
    "ðŸŒ² Keeps it compact.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 5. **`max_features`**\n",
    "\n",
    "> Randomly select only some features per split.\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "DecisionTreeClassifier(max_features=\"sqrt\")\n",
    "```\n",
    "\n",
    "ðŸŒˆ Adds randomness â†’ good for ensembles too.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 6. **Pruning** (Post-training trimming)\n",
    "\n",
    "In `sklearn`, use **Cost Complexity Pruning**:\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "DecisionTreeClassifier(ccp_alpha=0.01)\n",
    "```\n",
    "\n",
    "âœ‚ï¸ Removes branches that donâ€™t improve performance.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 7. **Cross-validation**\n",
    "\n",
    "> Tune hyperparameters using GridSearchCV or RandomizedSearchCV to find\n",
    "> sweet spot ðŸŽ¯\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### ðŸ”® BONUS: Use Random Forests ðŸŒ³ðŸŒ³ðŸŒ³\n",
    "\n",
    "Random Forest = **lots of shallow, random trees**\n",
    "\n",
    "âœ… Way more resistant to overfitting than a single deep tree.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## ðŸ§  TL;DR Cheat Sheet:\n",
    "\n",
    "| Parameter           | What it controls | Helps with     |\n",
    "|---------------------|------------------|----------------|\n",
    "| `max_depth`         | Tree depth       | Complexity     |\n",
    "| `min_samples_split` | When to split    | Over-splitting |\n",
    "| `min_samples_leaf`  | Small leaves     | Noise          |\n",
    "| `ccp_alpha`         | Post-pruning     | Tiny tweaks    |\n",
    "| `max_leaf_nodes`    | Limits decisions | Simplicity     |"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
