{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### **Filter Methods**\n",
    "\n",
    "### Code for Filter Methods on Iris\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Let's check correlation first\n",
    "corr_matrix = X.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# 1. ANOVA F-test (for numeric target)\n",
    "anova_selector = SelectKBest(score_func=f_classif, k='all')  # Select all features for now\n",
    "anova_selector.fit(X, y)\n",
    "anova_scores = pd.DataFrame(anova_selector.scores_, index=X.columns, columns=[\"ANOVA Score\"])\n",
    "print(\"ANOVA F-test scores:\")\n",
    "print(anova_scores.sort_values(by=\"ANOVA Score\", ascending=False))\n",
    "\n",
    "# 2. Chi-Square (usually for categorical data but let's run it for example)\n",
    "# First, we need to discretize the data since chi-square works with discrete categories\n",
    "X_discretized = pd.cut(X['sepal length (cm)'], bins=5)  # Example: discretizing 'sepal length'\n",
    "chi2_selector = SelectKBest(score_func=chi2, k='all')\n",
    "chi2_selector.fit(X_discretized.values.reshape(-1, 1), y)\n",
    "chi2_scores = pd.DataFrame(chi2_selector.scores_, index=[\"Discretized Sepal Length\"], columns=[\"Chi2 Score\"])\n",
    "print(\"\\nChi-Square scores:\")\n",
    "print(chi2_scores)\n",
    "\n",
    "# 3. Correlation (use absolute correlation with the target)\n",
    "correlation_with_target = X.apply(lambda x: x.corr(pd.Series(y)))\n",
    "print(\"\\nCorrelation with target:\")\n",
    "print(correlation_with_target.sort_values(ascending=False))\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Breakdown of the Code:\n",
    "\n",
    "1.  **Correlation Matrix**: Visualizes correlations between all features\n",
    "    (but works best with continuous data).\n",
    "\n",
    "2.  **ANOVA F-test**: Scores features for their relevance with the\n",
    "    target.\n",
    "\n",
    "3.  **Chi-Square**: We discretize one of the features for demonstration\n",
    "    and check feature relevance with the target.\n",
    "\n",
    "    (Note: Chi-square typically works better with **categorical\n",
    "    features**.)\n",
    "\n",
    "4.  **Correlation with Target**: Finds how much each feature correlates\n",
    "    with the target.\n",
    "\n",
    "### **Wrapper Methods**\n",
    "\n",
    "## 1. Recursive Feature Elimination (RFE)\n",
    "\n",
    "> “Start with all features, recursively drop the least important one.”\n",
    "\n",
    "### Use case: Works with any model like Logistic Regression, SVM, RandomForest.\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Logistic Regression model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# RFE with 2 features\n",
    "rfe = RFE(model, n_features_to_select=2)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Print results\n",
    "print(\"Selected Features (RFE):\")\n",
    "print(X.columns[rfe.support_])\n",
    "print(\"Ranking of features:\")\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## 2. Forward Selection\n",
    "\n",
    "> “Start with nothing, add features one by one that improve\n",
    "> performance.”\n",
    "\n",
    "### Use case: Good when you suspect *few features matter*.\n",
    "\n",
    "We’ll use `SequentialFeatureSelector` from `mlxtend`:\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Forward selection\n",
    "sfs = SFS(LogisticRegression(max_iter=200),\n",
    "          k_features=2,\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring='accuracy',\n",
    "          cv=5)\n",
    "\n",
    "sfs = sfs.fit(X_train.values, y_train)\n",
    "\n",
    "print(\"Selected Features (Forward):\", [X.columns[i] for i in sfs.k_feature_idx_])\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## 3. Backward Elimination\n",
    "\n",
    "> “Start with all features, drop the worst one at a time.”\n",
    "\n",
    "### Same idea as RFE, but this version removes based on performance drop.\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "# Backward Selection (change forward=False)\n",
    "sbs = SFS(LogisticRegression(max_iter=200),\n",
    "          k_features=2,\n",
    "          forward=False,\n",
    "          floating=False,\n",
    "          scoring='accuracy',\n",
    "          cv=5)\n",
    "\n",
    "sbs = sbs.fit(X_train.values, y_train)\n",
    "\n",
    "print(\"Selected Features (Backward):\", [X.columns[i] for i in sbs.k_feature_idx_])\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## 4. Exhaustive Search\n",
    "\n",
    "> “Try every possible combo of features. YES, ALL OF THEM.”\n",
    "\n",
    "**Slow as heck**. Good only for small datasets like Iris.\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "\n",
    "efs = EFS(LogisticRegression(max_iter=200),\n",
    "          min_features=2,\n",
    "          max_features=2,\n",
    "          scoring='accuracy',\n",
    "          cv=5)\n",
    "\n",
    "efs = efs.fit(X_train.values, y_train)\n",
    "\n",
    "print(\"Best feature combo (Exhaustive):\", [X.columns[i] for i in efs.best_idx_])\n",
    "```\n",
    "\n",
    "### **Embedded Methods**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## 1. **L1 Regularization (Lasso)**\n",
    "\n",
    "> Shrinks irrelevant feature coefficients to zero — automatic feature\n",
    "> kill switch 🔪\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Convert to binary classification (0 vs 1 only for Lasso)\n",
    "scalar=StandardScaler()\n",
    "x_scaled=scalar.fit_transform(x)\n",
    "\n",
    "# Fit Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(x_scaled, y_bin)\n",
    "\n",
    "# Coefficients\n",
    "coef = pd.Series(lasso.coef_, index=X.columns)\n",
    "print(\" Lasso Coefficients:\")\n",
    "print(coef)\n",
    "\n",
    "print(\"\\n Selected Features (non-zero):\")\n",
    "print(coef[coef != 0].index.tolist())\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## 2. **Tree-Based Models (Random Forest)**\n",
    "\n",
    "> Use built-in feature importance to rank & select features.\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Fit Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Feature importance\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\" Random Forest Feature Importances:\")\n",
    "print(importances.sort_values(ascending=False))\n",
    "\n",
    "print(\"\\n Selected Features (importance > 0.15):\")\n",
    "print(importances[importances > 0.15].index.tolist())\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## 3. **ElasticNet**\n",
    "\n",
    "> Mix of L1 (sparse) and L2 (smooth) — best of both worlds\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Fit ElasticNet on binary\n",
    "enet = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio = mix level\n",
    "enet.fit(X_bin, y_bin)\n",
    "\n",
    "enet_coef = pd.Series(enet.coef_, index=X.columns)\n",
    "print(\" ElasticNet Coefficients:\")\n",
    "print(enet_coef)\n",
    "\n",
    "print(\"\\n Selected Features (non-zero):\")\n",
    "print(enet_coef[enet_coef != 0].index.tolist())\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## 4. **Linear Model Coefficients (with regularization)**\n",
    "\n",
    "> You can also peek at coefficients directly from a Logistic Regression!\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Binary again\n",
    "logreg = LogisticRegression(penalty='l2', solver='liblinear')\n",
    "logreg.fit(X_bin, y_bin)\n",
    "\n",
    "log_coef = pd.Series(logreg.coef_[0], index=X.columns)\n",
    "print(\" Logistic Regression Coefficients:\")\n",
    "print(log_coef)\n",
    "\n",
    "print(\"\\n Selected Features (abs coef > 0.5):\")\n",
    "print(log_coef[log_coef.abs() > 0.5].index.tolist())\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### **Hybrid Methods**\n",
    "\n",
    "## Iris Dataset – Hybrid in Action\n",
    "\n",
    "``` python\n",
    "python\n",
    "CopyEdit\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# STEP 1: Filter Method (ANOVA)\n",
    "filter_selector = SelectKBest(score_func=f_classif, k=3)\n",
    "X_filter = filter_selector.fit_transform(X, y)\n",
    "\n",
    "# Get selected feature names\n",
    "mask = filter_selector.get_support()\n",
    "filtered_features = X.columns[mask]\n",
    "print(\"Filter Selected Features:\", filtered_features.tolist())\n",
    "\n",
    "# STEP 2: Wrapper Method (RFE with Logistic Regression)\n",
    "model = LogisticRegression(max_iter=200)\n",
    "rfe_selector = RFE(model, n_features_to_select=2)\n",
    "X_rfe = rfe_selector.fit_transform(X_filter, y)\n",
    "\n",
    "final_mask = rfe_selector.get_support()\n",
    "final_features = filtered_features[final_mask]\n",
    "print(\" Final Hybrid Selected Features:\", final_features.tolist())\n",
    "```"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
